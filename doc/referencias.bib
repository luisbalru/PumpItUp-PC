@article{knn,
	title = {Efficient algorithms for mining outliers from large data sets},
	url = {https://dl.acm.org/citation.cfm?id=335437},
	author = {Sridhar Ramaswamy, Rajeev Rastogi, Kyuseok Shim},
	date = {2000-05-15}
}

@article{autoencoder,
	title = {A practical tutorial on AutoEncoders for nonlinear feature fusion: Taxonomy, models, software and guidelines.},
	author = {David Charte, Francisco Charte, Salvador García, María J del Jesus, Francisco Herrera},
	date = {2017}
}

@article{ssma,
	author = {Salvador García, José Ramón Cano, Francisco Herrera},
	year = {2008},
	title = {A memetic algorithm for evolutionary prototype selection: A scaling up approach}
}

@article{smote,
	author = {Chawla, Nitesh and Bowyer, Kevin and Hall, Lawrence and Kegelmeyer, W.},
	year = {2002},
	month = {01},
	pages = {321-357},
	title = {SMOTE: Synthetic Minority Over-sampling Technique},
	volume = {16},
	journal = {J. Artif. Intell. Res. (JAIR)},
	doi = {10.1613/jair.953}
}

@article{iterative_imputer,
   author = {Stef van Buuren and Karin Groothuis-Oudshoorn},
   title = {mice: Multivariate Imputation by Chained Equations in R},
   journal = {Journal of Statistical Software, Articles},
   volume = {45},
   number = {3},
   year = {2011},
   keywords = {},
   abstract = {The R package mice imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice, which extends the functionality of mice 1.0 in several ways. In mice, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. mice adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
   issn = {1548-7660},
   pages = {1--67},
   doi = {10.18637/jss.v045.i03},
   url = {https://www.jstatsoft.org/v045/i03}
}

@article{ASADI201619,
title = "RipMC: RIPPER for Multiclass Classification",
journal = "Neurocomputing",
volume = "191",
pages = "19 - 33",
year = "2016",
issn = "0925-2312",
doi = "https://doi.org/10.1016/j.neucom.2016.01.010",
url = "http://www.sciencedirect.com/science/article/pii/S0925231216000576",
author = "Shahrokh Asadi and Jamal Shahrabi",
keywords = "RIPPER, Multiclass classification, Rule learning, Pruning",
abstract = "A major challenge in extending RIPPER for multiclass classification problems is the order of learning the classes. In this paper, RIPPER for Multiclass Classification (RipMC) is presented, which extends several aspects of RIPPER. In RipMC, all classes are initially given an equal opportunity with a Parallel Rule Learning (PRL) to generate their best rules in a global search, causing the rules in the decision list to be reordered, which improves performance in classifying new instances. Next, the most complex and costly class, which will be set as the default class in the subsequent execution of the algorithm, is identified according to a new measure called MaxDL. Finally, a new rule evaluation measure, namely LogLaplace, is presented for better pruning of the rules. The performance of the proposed algorithm and RIPPER is compared using 18 data sets. Experimental results show that RipMC significantly outperforms the original RIPPER."
}

@book{pca,
   title={Principal Component Analysis},
   author={Jolliffe, I.T. and Springer-Verlag},
   isbn={9780387954424},
   lccn={2002019560},
   series={Springer Series in Statistics},
   url={https://books.google.es/books?id=\_olByCrhjwIC},
	 year={2002},
	 publisher={Springer}
}

@MISC{kpca,
    author = {Bernhard Schölkopf and Alexander Smola and Klaus-Robert Müller},
    title = {Nonlinear component analysis as a kernel eigenvalue problem},
    year = {1996}
}

@article{ipf,
	author = {Sáez, José A. and Galar, Mikel and Luengo, Julián and Herrera, Francisco},
	year = {2015},
	month = {05},
	pages = {},
	title = {INFFC: An iterative class noise filter based on the fusion of classifiers with noise sensitivity control},
	volume = {27},
	journal = {Information Fusion},
	doi = {10.1016/j.inffus.2015.04.002}
}

@inproceedings{adasyn,
	author = {He, Haibo and Bai, Yang and Garcia, Edwardo and Li, Shutao},
	year = {2008},
	month = {07},
	pages = {1322 - 1328},
	title = {ADASYN: Adaptive Synthetic Sampling Approach for Imbalanced Learning},
	journal = {Proceedings of the International Joint Conference on Neural Networks},
	doi = {10.1109/IJCNN.2008.4633969}
}

@ARTICLE{allknn,
	author={},
	journal={IEEE Transactions on Systems, Man, and Cybernetics},
	title={An Experiment with the Edited Nearest-Neighbor Rule},
	year={1976},
	volume={SMC-6},
	number={6},
	pages={448-452},
	keywords={Statistical analysis;Testing;Physics;NASA;Sampling methods;Computer simulation;Computer science;Density functional theory;Prototypes;Nearest neighbor searches},
	doi={10.1109/TSMC.1976.4309523},
	ISSN={2168-2909},
	month={June}
}

@article{enn,
  title={Asymptotic Properties of Nearest Neighbor Rules Using Edited Data},
  author={Dennis L. Wilson},
  journal={IEEE Trans. Systems, Man, and Cybernetics},
  year={1972},
  volume={2},
  pages={408-421}
}

@misc{pyod,
	title={Python Outlier Detection},
	url={https://pyod.readthedocs.io/en/latest/}
}

@misc{pydml,
	title={Python Distance Metric Learning},
	url={https://pydml.readthedocs.io/en/latest/}
}
